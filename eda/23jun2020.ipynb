{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "\n",
    "DATA_FOLDER = '..//data//'\n",
    "d_parser = lambda x: pd.datetime.strptime(x,'%Y-%m-%d')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub = pd.read_csv(os.path.join(DATA_FOLDER,'sample_submission.csv'))\n",
    "df_stv        = pd.read_csv(os.path.join(DATA_FOLDER,'sales_train_validation.csv'))\n",
    "df_ste        = pd.read_csv(os.path.join(DATA_FOLDER,'sales_train_evaluation.csv'))\n",
    "df_prices     = pd.read_csv(os.path.join(DATA_FOLDER,'sell_prices.csv'))\n",
    "df_calander   = pd.read_csv(os.path.join(DATA_FOLDER,'calendar.csv'), parse_dates=[\"date\"], date_parser=d_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Sample Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important things to be noted about the Submission file**\n",
    "* The submission file is a format, which we have to follow when putting our submissions\n",
    "* The total number of rows in the submission files will be (total_items x total_stores x last 28 days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **It is quite clear now that the columns which start from d_ are representing days.**\n",
    "* **We need to convert those days into rows, so that we should be able to process them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for the Uniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Evaluation Dataset: \\n', df_ste.agg({'id':'nunique','item_id':'nunique', 'store_id':'nunique'}))\n",
    "print('\\n')\n",
    "print('Validation Dataset: \\n', df_stv.agg({'id':'nunique','item_id':'nunique', 'store_id':'nunique'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, the unique ID in both evaluation and validation is unique_item_id X unique_store_id**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Days in the Evaluation and Validation Datasets \n",
    "* The column format to be converted into row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ste.iloc[:, 6:].T\n",
    "dates_s = df_calander[\"date\"].values\n",
    "ids_s = df_ste[\"id\"].values\n",
    "df.columns = ids_s\n",
    "\n",
    "dfs = []\n",
    "for col in tqdm_notebook(df.columns):\n",
    "    _df = df[[col]].reset_index(drop=True)\n",
    "    _dr = _df.rename(columns={col:'qty'})\n",
    "    _df['id'] = col\n",
    "    dfs.append(_df)\n",
    "    \n",
    "df_tidy_ste = pd.concat(dfs)    \n",
    "df_tidy_ste['date'] = dates_s\n",
    "\n",
    "#saving the dataframe\n",
    "#df_tidy_ste.to_csv('df_ste_tidy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy_ste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy_ste.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows = df_ste.melt(\n",
    "id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "var_name ='d',\n",
    "value_name ='target'\n",
    ")\n",
    "\n",
    "df_stv_rows = df_stv.melt(\n",
    "id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "var_name ='d',\n",
    "value_name ='target'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stv_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the colum d has a suffix d_ , we can remove that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows['d']   = df_ste_rows['d'].apply(lambda x: x.replace('d_',''))\n",
    "df_ste_rows['d']   = df_ste_rows['d'].astype('int16')\n",
    "\n",
    "df_stv_rows['d']   = df_stv_rows['d'].apply(lambda x: x.replace('d_',''))\n",
    "df_stv_rows['d']   = df_stv_rows['d'].astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**checking for the rows after conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ste_rows), len(df_stv_rows), len(df_ste), len(df_stv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**as we can see that there are 60 million rows, we have to do some serious down casting here...**\n",
    "* I am also beginning to think that there must be a way to manage the data without having to do **melt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stv_rows.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the difference between the Evaluation and Validation Sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_d_in_e = df_ste_rows.d.max()\n",
    "max_d_in_v = df_stv_rows.d.max()\n",
    "\n",
    "d = 1449\n",
    "s = df_ste_rows[(df_ste_rows.d==d) & (df_ste_rows.state_id=='CA')]['target'].sum()\n",
    "t = df_stv_rows[(df_stv_rows.d==d) & (df_stv_rows.state_id=='CA')]['target'].sum()\n",
    "\n",
    "\n",
    "print(f'Last day in evaluation: {df_ste_rows.d.max()} and last day in Validation: {df_stv_rows.d.max()}, means 28 days more')\n",
    "print(f'Evaluation Dataset , total sales for day {d} is {s}, While in Validation it is {t}')\n",
    "print(f'max for validation is {max_d_in_v} and max in evaluation is {max_d_in_e}')\n",
    "print(f'total additional days in evaluation are {max_d_in_e - max_d_in_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This means that both data sets are same, and we have to train our model on validation dataset, and** \n",
    "\n",
    "### Step-1\n",
    "* train our model on validation dataset which is until 1913\n",
    "* predict for  1914 + 28\n",
    "* evaluate the performance of our dataset from the evaluate dataset, as these dates are available.\n",
    "\n",
    "### Step-2 (final predictions)\n",
    "* train our model on evaluation dataset which is until 1941\n",
    "* predict for 1942 + 28\n",
    "* submit to kaggle\n",
    "\n",
    "<font color=red> Or rather we don't use the validation data at all, and extract (last 28 days) from evaluation for test</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting rid of validation data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_stv, df_stv_rows\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.d.max(), df_ste_rows.target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.d   = df_ste_rows.d.astype('int16')\n",
    "df_ste_rows.target   = df_ste_rows.target.astype('int16')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ste_rows.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing for Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calander.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weeks_2015 = df_calander[df_calander.year==2015]['wm_yr_wk'].sort_values().nunique()\n",
    "print('We have ',df_calander[df_calander.year==2015]['wm_yr_wk'].sort_values().nunique(), ' weeks in 2015\\n')\n",
    "df_calander[df_calander.year==2015]['wm_yr_wk'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calander[(df_calander.year==2015) &\n",
    "           (df_calander.wm_yr_wk==11450)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_ste_rows[(df_ste_rows.d==1443) & (df_ste_rows.state_id=='CA')]['target'].sum()\n",
    "print(f'So in california state, we had {t} items sold on day 1443')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snap\n",
    "\n",
    "There are 3 binary variables with a prefix \"snap_\" plus the state name.\n",
    "\n",
    "snapCA, snapTX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n",
    "\n",
    "For those who is not familiar with SNAP like me;\n",
    "\"The United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP). SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products. In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.\"\n",
    "Source: https://www.fns.usda.gov/snap/supplemental-nutrition-assistance-program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**there is not point in keeping a prefix of d_ with the d column as we all know that this is a day number sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calander['d'] = df_calander['d'].apply(lambda x: x.replace('d_',''))\n",
    "df_calander['d'] = df_calander['d'].astype('int16')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Calendar and Sales (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain = df_ste_rows.merge(df_calander[['date','wm_yr_wk','wday','d','month','year']], on=['d'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying the merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_ste_rows[(df_ste_rows.d==1443) & (df_ste_rows.state_id=='CA')]['target'].sum()\n",
    "print(f'So in california state, we had {t} items sold on day 1443')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dfmain[(dfmain.d==1443) & (dfmain.state_id=='CA')]['target'].sum()\n",
    "print(f'So in california state, we had {s} items sold on day 1443')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the unwanted data sets\n",
    "del df_ste,df_ste_rows, df_calander\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain[(dfmain.d==1443)][['date','d','wm_yr_wk']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**so, this is basically item prices on a particular week in a particular store**\n",
    "* We can easily join the item prices to the main data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain = dfmain.merge(df_prices, how='left', on=['store_id','item_id','wm_yr_wk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Group by for Item and store for lags\n",
    "\n",
    "* what is the sales of a particular item across the country on a particular day\n",
    "    * That will be used as a lagged feature\n",
    "* what is the performance of a particular department across the country on a particular day \n",
    "    * That too, can be used as a lagged feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by item\n",
    "gb_item  = dfmain.groupby(['item_id','d'], as_index=False).target.sum()\n",
    "gb_item.rename(columns={'target':'target_item'}, inplace=True)    \n",
    "\n",
    "# by dept\n",
    "gb_dept  = dfmain.groupby(['dept_id','d'], as_index=False).target.sum()\n",
    "gb_dept.rename(columns={'target':'target_dept'}, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the item groupped target and store groupped target as feature\n",
    "dfmain = dfmain.merge(gb_dept, how='left', on=['dept_id','d']).fillna(0)\n",
    "dfmain = dfmain.merge(gb_item, how='left', on=['item_id','d']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gb_dept, gb_item\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### down casting, label encoding and removing unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all the ids to label encoded values\n",
    "le = LabelEncoder()\n",
    "dfmain['dept_id_code'] = le.fit_transform(dfmain.dept_id)\n",
    "dfmain['cat_id_code'] = le.fit_transform(dfmain.cat_id)\n",
    "dfmain['store_id_code'] = le.fit_transform(dfmain.store_id)\n",
    "dfmain['state_id_code'] = le.fit_transform(dfmain.state_id)\n",
    "dfmain['item_id_code'] = le.fit_transform(dfmain.item_id)\n",
    "\n",
    "#deleting all such columns\n",
    "dfmain.drop(['dept_id','cat_id','store_id','state_id','item_id'], axis=1, inplace=True)\n",
    "\n",
    "dfmain.dept_id_code.max(), dfmain.cat_id_code.max(),dfmain.store_id_code.max(),dfmain.item_id_code.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.item_id_code.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.wm_yr_wk     = dfmain.wm_yr_wk.astype('int16')\n",
    "dfmain.wday         = dfmain.wday.astype('int8')\n",
    "dfmain.month        = dfmain.month.astype('int8')\n",
    "dfmain.year         = dfmain.year.astype('int16')\n",
    "dfmain.sell_price   = dfmain.sell_price.astype('float16')\n",
    "dfmain.dept_id_code = dfmain.dept_id_code.astype('int8')\n",
    "dfmain.cat_id_code  = dfmain.cat_id_code.astype('int8')\n",
    "dfmain.store_id_code= dfmain.store_id_code.astype('int8')\n",
    "dfmain.state_id_code= dfmain.state_id_code.astype('int8')\n",
    "dfmain.item_id_code= dfmain.item_id_code.astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete the date as well, as it is not needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but before deleting the date, we may add dofm (day of month)\n",
    "def get_d_of_m(df):\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "dfmain['dom'] = dfmain['date'].apply(get_d_of_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.drop(['date'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain['sell_price'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding Means\n",
    "\n",
    "'''\n",
    "in future feature sets, we should be adding \n",
    "1 - store_state_target_mean \n",
    "2 - store_dept_target_mean etc \n",
    "3 - wday_target_mean\n",
    "4 - month_target_mean\n",
    "  - others\n",
    "'''\n",
    "\n",
    "def add_mean(dfmain, col):\n",
    "    mean_attrib = col + '_target_mean'\n",
    "    mean_values = dfmain.groupby(col).target.mean()\n",
    "    dfmain[mean_attrib] = dfmain[col].map(mean_values)\n",
    "    return dfmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain = add_mean(dfmain,'store_id_code')\n",
    "dfmain = add_mean(dfmain,'cat_id_code')\n",
    "dfmain = add_mean(dfmain,'state_id_code')\n",
    "dfmain = add_mean(dfmain,'item_id_code')\n",
    "dfmain = add_mean(dfmain,'dept_id_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain[['store_id_code','store_id_code_target_mean']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain[['cat_id_code','cat_id_code_target_mean']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain[['dept_id_code','dept_id_code_target_mean']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the following attributes, since we've already added their means\n",
    "df_train = dfmain.drop(['cat_id_code','state_id_code','dept_id_code','wm_yr_wk','wday','sell_price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfmain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dept_id_code_target_mean = df_train.dept_id_code_target_mean.astype('float16')\n",
    "df_train.item_id_code_target_mean = df_train.item_id_code_target_mean.astype('float16')\n",
    "df_train.state_id_code_target_mean= df_train.state_id_code_target_mean.astype('float16')\n",
    "df_train.store_id_code_target_mean= df_train.store_id_code_target_mean.astype('float16')\n",
    "df_train.cat_id_code_target_mean  = df_train.cat_id_code_target_mean.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.year > 2013] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(df, shift_range,index_cols, lag_cols, exception_cols):\n",
    "    cols_to_rename = list(df.columns.difference(index_cols + exception_cols)) \n",
    "    \n",
    "    print('Columns to rename : ',cols_to_rename)\n",
    "    \n",
    "    print(index_cols + cols_to_rename)\n",
    "\n",
    "    for day_shift in tqdm_notebook(shift_range):\n",
    "        train_shift = df[index_cols + cols_to_rename].copy()\n",
    "        print('copied to train_shift')\n",
    "        train_shift['d'] = train_shift['d'] + day_shift\n",
    "        \n",
    "        print(f'performed the shifting of {day_shift}')\n",
    "\n",
    "        foo = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "        \n",
    "        \n",
    "        df = pd.merge(df, train_shift, on=index_cols, how='left').fillna(0)\n",
    "        print('--------------- ', train_shift ,'  ---------------')\n",
    "        print('performed the merge')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the index columns where we will merge the lags will be**\n",
    "* d\n",
    "* item_id_code\n",
    "* store_id_code\n",
    "\n",
    "the obvious reason to chose these three is the fact that, id column is a concatenation of dept_id and store_id\n",
    "\n",
    "**while one thing, that I am not able to findout is that the id has either _validation or _evaluation suffix.**\n",
    "\n",
    "whether we are going to use _validation suffice in the id while submitting, I am not sure but once we submit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_enc_cols = [col for col in df_train.columns if 'mean' in str(col)]\n",
    "exception_cols = mean_enc_cols + ['cat_id', 'date', 'day', 'id','sell_price', 'snap_CA', 'snap_TX', 'snap_WI',\n",
    "                                  'state_id', 'wday', 'wm_yr_wk','year','month','dom']\n",
    "\n",
    "index_cols = ['store_id_code','item_id_code','d']\n",
    "lag_cols = ['target']\n",
    "shift_range = [x for x in range(1,29)]\n",
    "\n",
    "df_train = add_lags(df_train,shift_range,index_cols,lag_cols,exception_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train.item_id_code==1) & (df_train.store_id_code==1)][['d','target_dept','target_dept_lag_1','target_dept_lag_2',\n",
    "                                                                   'target_dept_lag_3','target_dept_lag_4',\n",
    "                                                                   'target_dept_lag_5']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train.item_id_code==1) & (df_train.store_id_code==1)][['d','target_item','target_item_lag_1','target_item_lag_2',\n",
    "                                                                   'target_item_lag_3','target_item_lag_4',\n",
    "                                                                   'target_item_lag_5']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.target_dept.max(), df_train.target_item.max(), df_train.target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all the lags to be int16\n",
    "lag_cols = [col for col in df_train.columns if 'lag' in str(col)]\n",
    "for col in lag_cols:\n",
    "    df_train[col] = df_train[col].astype('int16')  \n",
    "    \n",
    "df_train.info()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Idea\n",
    "\n",
    "**Since we have to predict 28 days, 3049 items, and 10 stores**\n",
    "* Days = 28\n",
    "* Items = 3049\n",
    "* Stores = 10\n",
    "\n",
    "Total Predictions = 28 x 3049 x 10 = 853,720\n",
    "\n",
    "**Let's see if this is roughly the sum of each month entries in our dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**so, this clears the logic, as we can see that feb-2014 and feb-2015 have similar number of items**\n",
    "\n",
    "**Let's also suppose that we are to predict only one day**\n",
    "\n",
    "Then the equation would be : \n",
    "    \n",
    "    * 3049 x 10 = 30,490\n",
    "    \n",
    "So, \n",
    "    There can be two ways to do that\n",
    "\n",
    "    \n",
    "* **First Procedure** \n",
    "    Train for all days (huge dataset and huge model size)\n",
    "\n",
    "    for day in range(1,29):\n",
    "        predict([item,day])\n",
    "        \n",
    "        \n",
    "* **Second Procedure** \n",
    "\n",
    "    * seperate each day and create 28 data sets\n",
    "    for day in range(1,29)\n",
    "        * load_data(day)\n",
    "        * train data\n",
    "        * predict(for day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
